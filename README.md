# ğŸ“š RAG Personal Assistant

A personal Retrieval-Augmented Generation (RAG) system that answers user questions over your own documents using either cloud or local LLMs.  

This project is designed to **demonstrate production-ready patterns** for building an end-to-end RAG pipeline, suitable for personal use, local deployment.

---

## ğŸš€ Features

âœ… Ingest PDF documents into a vector database  
âœ… Semantic chunking with metadata (e.g., source path)  
âœ… Local or OpenAI Embeddings (configurable)  
âœ… FAISS vector store for offline search  
âœ… Retrieval + Augmented Generation pipeline  
âœ… Supports:
- OpenAI GPT models (fallback)
- Local GGUF LLMs via llama-cpp-python

âœ… Modular and configurable project structure  
âœ… CLI and notebook demos

---

## âš™ï¸ Architecture

[ PDF ] --> [ Chunking & Embeddings ] --> [ FAISS Vector Store ]
â†˜
[ Retrieval ]
â†˜
[ LLM ]
â†˜
[ Answer ]

## ğŸ—‚ï¸ Project Structure
rag-personal-assistant/
â”œâ”€â”€ models/ # (Put your downloaded GGUF models here)
â”œâ”€â”€ vector_db/ # Local FAISS DB
â”œâ”€â”€ data/ # Your PDFs
â”œâ”€â”€ notebooks/ # Interactive notebooks
â””â”€â”€ src/
â””â”€â”€ rag_assistant/
â”œâ”€â”€ config.py
â”œâ”€â”€ ingest.py
â”œâ”€â”€ retrieve.py
â”œâ”€â”€ rag_chain.py
â”œâ”€â”€ llm_loader.py
â””â”€â”€ utils.py

## ğŸ’¾ Local Model Support

This project supports local LLM inference using [llama-cpp-python](https://github.com/abetlen/llama-cpp-python). 

**Recommended model:**  
- [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)

âœ… Recommended quantization: `Q4_K_M` (suitable for CPU-only inference)

### Download Instructions

1ï¸âƒ£ Go to [Hugging Face link](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)  
2ï¸âƒ£ Download `mistral-7b-instruct-v0.1.Q4_K_M.gguf`  
3ï¸âƒ£ Place it in your local `models/` folder:
rag-personal-assistant/
â””â”€â”€ models/
â””â”€â”€ mistral-7b-instruct-v0.1.Q4_K_M.gguf

âœ… *Note: This model file is large (several GB) and is NOT included in this repo.*

---

## ğŸ§ª Quickstart

1ï¸âƒ£ Install dependencies:

```bash
pip install -r requirements.txt

2ï¸âƒ£ Set your .env file:

3ï¸âƒ£ Ingest your documents:
python -m src.rag_assistant.ingest

4ï¸âƒ£ Retrieve and answer:
python -m src.rag_assistant.retrieve

5ï¸âƒ£ Use in notebooks for rapid prototyping!

âš™ï¸ Configuration
All settings are managed in src/rag_assistant/config.py:

* Embedding Model Type (OpenAI vs Local)

* OpenAI model + key

* Local GGUF model path

* FAISS DB path

* Verbose logging

Example:

LLM_TYPE = "local"   # "openai" or "local"
LOCAL_MODEL_PATH = PROJECT_ROOT / "models" / "mistral-7b-instruct-v0.1.Q4_K_M.gguf"

ğŸŒ OpenAI + Local Fallback
âœ¨ This project supports automatic fallback:

Primary: OpenAI GPT (if available)

Fallback: Local GGUF model (if OpenAI is rate-limited / unavailable)

ğŸ¤– Why This Project?
This is not "just calling an API" â€” it demonstrates:

âœ… Real retrieval-augmented generation pipeline
âœ… Local/offline-friendly architecture
âœ… Configurable multi-backend support
âœ… Practical engineering for real-world use cases

Perfect for:

Personal assistants

Enterprise knowledge bases

ğŸ“œ License
Code: MIT License
Content (docs, images): CC-BY-NC 4.0

ğŸ™ Acknowledgements
LangChain

llama-cpp-python

sentence-transformers

FAISS